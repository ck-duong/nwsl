{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWSL Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The National Women's Soccer League (NWSL) is the premier professional women's soccer league in the United States. In this repository, I will be scraping player and team data, from the NWSL website (www.nwslsoccer.com) and performing exploratory data analysis on the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports to run the code\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "In the subdirectory \"scraping\", there are two Python files written to scrape data from the official NWSL website: statscrape.py and teamscrape.py. \n",
    "\n",
    "The statscrape.py file scrapes player data from the Stats page of the website for each player in the league from 2016 through 2019, each of the years the league has existed and compiles them into csv files by year, entitled \"nwsl{}.csv\" for each year. \n",
    "\n",
    "The teamscrape.py file scrapes player data from the Team pages of the website for each team for each year the team has existed and compiles them into csv files by year, entitled \"position{}.csv\" for each year.\n",
    "\n",
    "In the following cell of code, I run these two files to create the csvs I will be working with in the rest of this notebook. Currently they are commented out since they only need to be run once to collect our data. However, I will note that the 2019 NWSL season is currently taking place, meaning that rerunning these files will get us the most up to date data. \n",
    "\n",
    "For this analysis, I will only be looking at the April and May statistics for the 2019 season, although my code will be able to work with future data as well since it will all be formatted in the same way. It is also worth nothing that as of June 2019, many NWSL teams are missing players who also serve on their national team (such as the USWNT, CANWNT, etc) due to the Women's World Cup occuring this summer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#py files to run to scrape the data from the NWSL page. Only need to run once.\n",
    "#!python ./scraping/statscrape.py\n",
    "#!python ./scraping/teamscrape.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Pre-Analysis\n",
    "#TODO: GOAL/ASSISTS PER GAME PERCENTAGES/RATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination(start_year, end_year):\n",
    "    \"\"\"\n",
    "    Combines the nwsl.csv and position.csv csvs for each\n",
    "    year in the given range ad\n",
    "    \n",
    "    :parameters:\n",
    "    start_year - integer indicating start year of data\n",
    "    end_year - integer indicating end year of data\n",
    "    \"\"\"\n",
    "    for i in range(start_year, end_year + 1):\n",
    "        nwsl_file = 'nwsl{}.csv'.format(i)\n",
    "        position_file = 'position{}.csv'.format(i)\n",
    "\n",
    "        nwsl = pd.read_csv(os.path.join('data', 'nwsl', nwsl_file))\n",
    "        position = pd.read_csv(os.path.join('data', 'position', position_file))\n",
    "        df = nwsl.merge(position, left_on='Player Name',\n",
    "                            right_on='Player', how = 'left').drop('Player', axis = 1)\n",
    "\n",
    "        name = 'full{}.csv'.format(i)\n",
    "        path = os.path.join('data', 'full', name)\n",
    "\n",
    "        df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination(2016, 2019) #run to join all of the nwsl/position csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/full/full2016.csv',\n",
       " 'data/full/full2017.csv',\n",
       " 'data/full/full2018.csv',\n",
       " 'data/full/full2019.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting all of the full.csv files in the subdirectory\n",
    "full_path = os.path.join('data', 'full')\n",
    "csvs = os.listdir(path = full_path)\n",
    "files = []\n",
    "#for loop to get all the full.csv paths\n",
    "for file in csvs:\n",
    "    fp = os.path.join(full_path, file)\n",
    "    files.append(fp)\n",
    "#for organization purposes later\n",
    "files.sort()\n",
    "files\n",
    "#use nwsl.files, full for with prediction later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwsl_2016 = pd.read_csv(files[0])\n",
    "nwsl_2017 = pd.read_csv(files[1])\n",
    "nwsl_2018 = pd.read_csv(files[2])\n",
    "nwsl_2019 = pd.read_csv(files[3]) #Training\n",
    "all_nwsl = [nwsl_2016, nwsl_2017, nwsl_2018, nwsl_2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(df):\n",
    "    \"\"\"\n",
    "    Calculates Goals per Game, Assists per Game, Shots per Game, \n",
    "    Proportion of Shots on Goal per Goal, Proportion of Shots on Goal, \n",
    "    and Proportion of Successful Penalty Kicks, for each player \n",
    "    in the dataset. Creates columns for these values in each dataframe.\n",
    "    \n",
    "    :parameters:\n",
    "    df - dataframe like nwsl.csv/full.csv with neceesary columns\n",
    "    \"\"\"\n",
    "    #calculating stats, self explanatory column names\n",
    "    df['Goals per Game'] = df['Goals']/df['Games Played']\n",
    "    df['Assists per Game'] = df['Assists']/df['Games Played']\n",
    "    df['Shots per Game'] = df['Shots']/df['Games Played']\n",
    "    df['Prop SoG'] = df['Shots on Goal']/df['Shots']\n",
    "    df['Shots per Goal'] = df['Goals']/df['Shots on Goal']\n",
    "    df['Prop Penalty'] = df['Penalty Kick Goals']/df['Penalty Kicks Attempted']\n",
    "    \n",
    "    int_cols = df.columns[2:].tolist()\n",
    "    int_cols.remove('Position')\n",
    "    for each in int_cols:\n",
    "        df[each] = df[each].astype(float)\n",
    "    \n",
    "    #May create a classifer for Position later, leaving nulls in this column\n",
    "    nonPos = df.loc[:, ~df.columns.isin(['Position'])].columns.tolist()\n",
    "    df[nonPos] = df[nonPos].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply above function to all dataframes in the list\n",
    "for each in all_nwsl:\n",
    "    calculate_stats(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NOTE: MISSINGNESS FOR POSITION DATA: SOME TEAMS DON'T POST THEIR FULL OLDER ROSTER IE CRS 2016 ONLY HAD 5 PLAYERS\n",
    "#PLAYER POSITION MISSINGNESS: SOME PLAYERS ARE TEMPS/HIRES AND NOT FULLY CONTRACTED or TRADES/IF A PLAYER TRADED TEAMS THEN THEIR OLD TEAM DISCARDS THEIR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the positions of each player, predict what position they play based on goals/assists/etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CREATE FEATURES FOR PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kNN', 0.4084507042253521),\n",
       " ('bayes', 0.5492957746478874),\n",
       " ('forest', 0.4788732394366197)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: nwsl_2019 has positions for all players due to its role as\n",
    "#most recent data on the league\n",
    "\n",
    "y = nwsl_2019['Position']\n",
    "features = nwsl_2019.drop(['Position', 'Team', 'Player Name'], axis=1)\n",
    "#put in a column transformer and onehotencode by team\n",
    "\n",
    "encoder = StandardScaler()\n",
    "X = encoder.fit_transform(features, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "neighbors = KNeighborsClassifier()\n",
    "neighbors.fit(X_train, y_train)\n",
    "neigh_pred = neighbors.predict(X_test)\n",
    "neigh_f1 = accuracy_score(y_true=y_test, y_pred=neigh_pred)\n",
    "\n",
    "bayesian = GaussianNB()\n",
    "bayesian.fit(X_train, y_train)\n",
    "bay_pred = bayesian.predict(X_test)\n",
    "bay_f1 = accuracy_score(y_true=y_test, y_pred=bay_pred)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=3,\n",
    "                                min_samples_split=20, min_samples_leaf=10)\n",
    "forest.fit(X_train, y_train)\n",
    "forest_pred = forest.predict(X_test)\n",
    "forest_f1 = accuracy_score(y_true=y_test, y_pred=forest_pred)\n",
    "values = ([('kNN', neigh_f1), ('bayes', bay_f1), ('forest', forest_f1)])\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
